{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.9.16)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.array, zip(*batch))\n",
    "        return state, action, reward, next_state, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_q_model(state_shape, action_space):\n",
    "    inputs = layers.Input(shape=state_shape)\n",
    "    layer1 = layers.Dense(64, activation='relu')(inputs)\n",
    "    layer2 = layers.Dense(64, activation='relu')(layer1)\n",
    "    action = layers.Dense(action_space, activation='linear')(layer2)\n",
    "    return models.Model(inputs=inputs, outputs=action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_shape, action_space):\n",
    "        self.state_shape = state_shape\n",
    "        self.action_space = action_space\n",
    "        self.model = create_q_model(state_shape, action_space)\n",
    "        self.target_model = create_q_model(state_shape, action_space)\n",
    "        self.replay_buffer = ReplayBuffer(10000)\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.1\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.batch_size = 64\n",
    "        self.gamma = 0.99\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        self.loss_function = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return random.randint(0, self.action_space - 1)\n",
    "        state_tensor = tf.convert_to_tensor(state)\n",
    "        state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "        action_probs = self.model(state_tensor, training=False)\n",
    "        return tf.argmax(action_probs[0]).numpy()\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.replay_buffer.buffer) < self.batch_size:\n",
    "            return\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "        future_rewards = self.target_model.predict(next_states)\n",
    "        updated_q_values = rewards + self.gamma * tf.reduce_max(future_rewards, axis=1) * (1 - dones)\n",
    "        masks = tf.one_hot(actions, self.action_space)\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_values = self.model(states)\n",
    "            q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "            loss = self.loss_function(updated_q_values, q_action)\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game Over! Score: 3\n",
      "Game Over! Score: 0\n",
      "Game Over! Score: 16\n",
      "Game Over! Score: 3\n"
     ]
    }
   ],
   "source": [
    "pygame.init()\n",
    "SCREEN_WIDTH, SCREEN_HEIGHT = 800, 600\n",
    "screen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT))\n",
    "FPS = 30\n",
    "clock = pygame.time.Clock()\n",
    "\n",
    "\n",
    "WHITE = (255, 255, 255)\n",
    "BLACK = (0, 0, 0)\n",
    "RED = (255, 0, 0)\n",
    "player_size = 50\n",
    "player_velocity = 5\n",
    "obstacle_width = 70\n",
    "obstacle_height = 50\n",
    "obstacle_speed = 5\n",
    "obstacle_frequency = 20\n",
    "\n",
    "\n",
    "\n",
    "state_shape = (4,)\n",
    "action_space = 2\n",
    "agent = DQNAgent(state_shape, action_space)\n",
    "\n",
    "\n",
    "running = True\n",
    "while running:\n",
    "    player_x = SCREEN_WIDTH // 2 - player_size // 2\n",
    "    obstacles = []\n",
    "    score = 0\n",
    "    total_steps = 0\n",
    "    game_active = True\n",
    "\n",
    "    while game_active:\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "                game_active = False\n",
    "\n",
    "        if not running:\n",
    "            break\n",
    "\n",
    "        screen.fill(BLACK)\n",
    "        closest_obstacle = min(obstacles, key=lambda x: x[1]) if obstacles else [0, 0, -1]\n",
    "        state = np.array([player_x, closest_obstacle[0], closest_obstacle[1], obstacle_speed]) / SCREEN_WIDTH\n",
    "        action = agent.act(state)\n",
    "\n",
    "\n",
    "        if action == 0 and player_x > 0:\n",
    "            player_x -= player_velocity\n",
    "        elif action == 1 and player_x < SCREEN_WIDTH - player_size:\n",
    "            player_x += player_velocity\n",
    "\n",
    "        if random.randint(0, obstacle_frequency) == 1:\n",
    "            obstacles.append([random.randint(0, SCREEN_WIDTH - obstacle_width), 0 - obstacle_height])\n",
    "\n",
    "        collision = False\n",
    "        for i in range(len(obstacles) - 1, -1, -1):\n",
    "            obstacles[i][1] += obstacle_speed\n",
    "            if obstacles[i][1] > SCREEN_HEIGHT:\n",
    "                obstacles.pop(i)\n",
    "                score += 1\n",
    "            elif pygame.Rect(player_x, SCREEN_HEIGHT - player_size - 10, player_size, player_size).colliderect(pygame.Rect(obstacles[i][0], obstacles[i][1], obstacle_width, obstacle_height)):\n",
    "                collision = True\n",
    "                break\n",
    "\n",
    "        player_rect = pygame.Rect(player_x, SCREEN_HEIGHT - player_size - 10, player_size, player_size)\n",
    "        pygame.draw.rect(screen, WHITE, player_rect)\n",
    "        for obstacle in obstacles:\n",
    "            pygame.draw.rect(screen, RED, (obstacle[0], obstacle[1], obstacle_width, obstacle_height))\n",
    "\n",
    "        pygame.display.flip()\n",
    "        clock.tick(FPS)\n",
    "\n",
    "        next_state = state\n",
    "        reward = -10 if collision else 1\n",
    "        done = collision\n",
    "\n",
    "        agent.replay_buffer.add(state, action, reward, next_state, done)\n",
    "        agent.replay()\n",
    "\n",
    "\n",
    "        if total_steps % 20 == 0:\n",
    "            agent.update_target_network()\n",
    "\n",
    "        total_steps += 1\n",
    "\n",
    "        if done:\n",
    "            print(f\"Game Over! Score: {score}\")\n",
    "            game_active = False\n",
    "            \n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
